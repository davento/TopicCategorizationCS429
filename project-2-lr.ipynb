{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3761d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import tqdm as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 69420\n",
    "np.random.seed(69420)\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook, ColumnDataSource\n",
    "from bokeh.layouts import row\n",
    "from bokeh.io import push_notebook\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc66673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = open('newsgrouplabels.txt', 'r')\n",
    "classes = []\n",
    "#print(\"The classes are\")\n",
    "for line in Y:\n",
    "    y = line.split()\n",
    "    classes.append(y)\n",
    "    #print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b1a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sparse_dataset(dataset_name: str, *args, **kwargs) -> sparse.csr_matrix:\n",
    "    with open(dataset_name, 'r') as f:\n",
    "        num_lines = sum(1 for _ in f)\n",
    "\n",
    "    sparse_row_list = []\n",
    "    with open(dataset_name, 'r') as f:\n",
    "        for row in tqdm.tqdm(f, total=num_lines):\n",
    "            data = np.fromstring(row, *args, **kwargs)\n",
    "            sparse_row = sparse.csr_matrix(data)\n",
    "            sparse_row_list.append(sparse_row)\n",
    "\n",
    "    data_matrix: sparse.csr_matrix\n",
    "    data_matrix = sparse.vstack(sparse_row_list)\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9a8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 12000/12000 [00:22<00:00, 527.97it/s]\n"
     ]
    }
   ],
   "source": [
    "Xy_raw: sparse.csr_matrix\n",
    "Xy_raw = read_sparse_dataset(\"training.csv\", sep=',', dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329c6e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 6774/6774 [00:13<00:00, 491.96it/s]\n"
     ]
    }
   ],
   "source": [
    "xTest: sparse.csr_matrix\n",
    "xTest = read_sparse_dataset(\"testing.csv\", sep=',', dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683c30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = Xy_raw[:, 1:-1].toarray()\n",
    "y_bow = Xy_raw[:, -1].toarray()\n",
    "xTest_bow = xTest[:, 1:].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b056b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformer.fit(X_bow)\n",
    "\n",
    "X_tfidf = transformer.transform(X_bow)\n",
    "xTest_tfidf = transformer.transform(xTest_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e112a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=25000)\n",
    "X_chi = selector.fit_transform(X_tfidf, y_bow)\n",
    "xTest_chi = selector.transform(xTest_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd31070",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_smote, Y_smote = smote.fit_resample(X_chi, y_bow.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "484f9531",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_smote)\n",
    "X_norm = sparse.csr_matrix(scaler.transform(X_smote))\n",
    "xTest_norm = sparse.csr_matrix(scaler.transform(xTest_chi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f60074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "y_cat = encoder.fit_transform(Y_smote.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aef8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(*, y, s):\n",
    "    e = 1e-8\n",
    "    return -np.sum(y * np.log(s + e)).mean()\n",
    "\n",
    "def multilogistic(*, z):\n",
    "    z = z - np.max(z, axis=0, keepdims=True)\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0, keepdims=True)\n",
    "\n",
    "def calculate_gradient(*, X, y, s):\n",
    "    return (y.T - s) @ X\n",
    "\n",
    "def predict(*, X, W):\n",
    "    return multilogistic(z=W @ X.T)\n",
    "\n",
    "def accuracy(*, y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f99ee92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, Xv, yv, \n",
    "                        W_init=None, epochs=1000, minEpoch=5, \n",
    "                        eta=1e-2, lambda_=1e-2, threshold=1e-6):\n",
    "    if W_init is None:\n",
    "        W = np.random.normal(0, 0.1, (y.shape[1], X.shape[1])) \n",
    "        W = np.hstack((np.zeros((y.shape[1],1)), W))\n",
    "    else:\n",
    "        W = W_init\n",
    "\n",
    "    X = sparse.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    Xv = sparse.hstack((np.ones((Xv.shape[0], 1)), Xv))\n",
    "    losses = []\n",
    "    lossesV = []\n",
    "    accuracies = []\n",
    "    accuraciesV = []\n",
    "\n",
    "#     output_notebook()\n",
    "\n",
    "#     source_loss = ColumnDataSource(data=dict(x=[], y_train=[], y_val=[]))\n",
    "#     source_acc = ColumnDataSource(data=dict(x=[], y_train=[], y_val=[]))\n",
    "\n",
    "#     p1 = figure(title=\"Loss\", x_axis_label='Epochs', y_axis_label='Loss', plot_width=400, plot_height=300)\n",
    "#     p1.line('x', 'y_train', source=source_loss, legend_label=\"Train Loss\", line_color=\"blue\")\n",
    "#     p1.line('x', 'y_val', source=source_loss, legend_label=\"Val Loss\", line_color=\"green\")\n",
    "\n",
    "#     p2 = figure(title=\"Accuracy\", x_axis_label='Epochs', y_axis_label='Accuracy', plot_width=400, plot_height=300)\n",
    "#     p2.line('x', 'y_train', source=source_acc, legend_label=\"Train Accuracy\", line_color=\"blue\")\n",
    "#     p2.line('x', 'y_val', source=source_acc, legend_label=\"Val Accuracy\", line_color=\"green\")\n",
    "\n",
    "#     layout = row(p1, p2)\n",
    "#     handle = show(layout, notebook_handle=True)\n",
    "\n",
    "    try:\n",
    "        #with epochs as t:\n",
    "        for i in range(epochs):\n",
    "\n",
    "            s = multilogistic(z=W @ X.T)  \n",
    "            st = multilogistic(z=W @ Xv.T)\n",
    "            gradient = calculate_gradient(X = X, y = y, s = s) - (lambda_ * W)\n",
    "\n",
    "            loss = calculate_loss(y = y, s = s.T) + (lambda_/2) * np.sum(W**2)\n",
    "            losses.append(loss)\n",
    "            lossV = calculate_loss(y = yv, s = st.T) + (lambda_/2) * np.sum(W**2)\n",
    "            lossesV.append(lossV)\n",
    "\n",
    "            accuracy_ = accuracy(y_true = np.argmax(y, axis=1), y_pred = np.argmax(predict(X=X,W=W), axis=0))\n",
    "            #t.set_postfix(loss=f\"{loss:.6f}\", accuracy=f\"{accuracy_:.6f}\")\n",
    "            accuracies.append(accuracy_)\n",
    "\n",
    "            accuracyV_ = accuracy(y_true = np.argmax(yv, axis=1), y_pred = np.argmax(predict(X=Xv,W=W), axis=0))\n",
    "            #t.set_postfix(lossV=f\"{lossV:.6f}\", accuracyV=f\"{accuracyV_:.6f}\")\n",
    "            accuraciesV.append(accuracyV_)\n",
    "\n",
    "            #accT = 0.0001\n",
    "            if i > minEpoch:\n",
    "                if np.abs(losses[-1] - losses[-2]) < threshold:# or np.abs(accuracies[-1] - accuracies[-2]) < accT:\n",
    "                    #print(\"Converged at epoch \", i)\n",
    "                    break\n",
    "\n",
    "            W = W + eta * gradient\n",
    "\n",
    "#                 # Update plots\n",
    "#                 source_loss.stream(dict(x=[i], y_train=[loss], y_val=[lossV]))\n",
    "#                 source_acc.stream(dict(x=[i], y_train=[accuracy_], y_val=[accuracyV_]))\n",
    "\n",
    "#                 # Redraw plots\n",
    "#                 push_notebook(handle=handle)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user at epoch \", i)\n",
    "\n",
    "    return W, losses, lossesV, accuracies, accuraciesV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c1a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_chi, X_val_chi, y_train_chi, y_val_chi = train_test_split(X_norm, y_cat, test_size=0.2, stratify=np.asarray(np.argmax(y_cat, axis=1)), random_state=seed) # type: ignore\n",
    "y_train_chi, y_val_chi = y_train_chi.toarray(), y_val_chi.toarray() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667ef618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at epoch  8\n"
     ]
    }
   ],
   "source": [
    "WvChi, lossesvChi, lossesVvChi, accuraciesChi, accuraciesVChi = logistic_regression(X_train_chi, y_train_chi, X_val_chi, y_val_chi, epochs = 1000, minEpoch = 5, eta = 1e-1, lambda_ = 1e-3, threshold = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98d2fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_chi_p = sparse.hstack((np.ones((X_val_chi.shape[0], 1)), X_val_chi))\n",
    "st = predict(X = X_val_chi_p, W = WvChi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1f6b8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.86      0.95      0.91       130\n",
      "           comp.graphics       0.94      0.78      0.85       130\n",
      " comp.os.ms-windows.misc       0.88      0.87      0.88       130\n",
      "comp.sys.ibm.pc.hardware       0.84      0.75      0.79       131\n",
      "   comp.sys.mac.hardware       0.81      0.85      0.83       130\n",
      "          comp.windows.x       0.92      0.88      0.90       130\n",
      "            misc.forsale       0.92      0.83      0.87       130\n",
      "               rec.autos       0.99      0.88      0.93       130\n",
      "         rec.motorcycles       0.93      0.96      0.94       130\n",
      "      rec.sport.baseball       0.97      0.97      0.97       130\n",
      "        rec.sport.hockey       0.98      0.95      0.96       130\n",
      "               sci.crypt       0.94      0.98      0.96       130\n",
      "         sci.electronics       0.85      0.88      0.86       130\n",
      "                 sci.med       0.89      0.95      0.92       130\n",
      "               sci.space       0.89      0.96      0.92       130\n",
      "  soc.religion.christian       0.91      0.93      0.92       130\n",
      "      talk.politics.guns       0.94      0.92      0.93       130\n",
      "   talk.politics.mideast       0.88      0.98      0.93       131\n",
      "      talk.politics.misc       0.85      0.90      0.87       131\n",
      "      talk.religion.misc       0.93      0.91      0.92       131\n",
      "\n",
      "                accuracy                           0.90      2604\n",
      "               macro avg       0.91      0.90      0.90      2604\n",
      "            weighted avg       0.91      0.90      0.90      2604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = np.argmax(y_val_chi, axis=1)  # Ground truth labels\n",
    "y_pred = np.argmax(st, axis=0)  # Predicted labels\n",
    "labels = list(np.array(classes)[:, 1])  # List of unique class labels\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c60b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xTest_norm_p = sparse.hstack((np.ones((xTest_norm.shape[0], 1)), xTest_norm))\n",
    "yPred = np.argmax(predict(W = WvChi, X = xTest_norm_p), axis=0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f89749",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(pd.DataFrame(yPred))\n",
    "predictions.columns = [\"class\"]\n",
    "predictions.index = np.arange(1, len(predictions) + 1)\n",
    "predictions.set_index(pd.Index(range(12001, 12001 + len(predictions))), inplace=True)\n",
    "predictions.index.name='id'\n",
    "predictions.to_csv(\"predicions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82aabac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta 1e-05, lambda 1e-05, Train Accuracy 0.9987519201228878, Validation Accuracy 0.5061443932411674, Train Loss 111.73329150534803, Validation Loss 15545.080277813255\n",
      "eta 1e-05, lambda 0.0001, Train Accuracy 0.9989439324116743, Validation Accuracy 0.5372503840245776, Train Loss 98.27532192031745, Validation Loss 14917.913094484617\n",
      "eta 1e-05, lambda 0.001, Train Accuracy 0.9987519201228878, Validation Accuracy 0.5272657450076805, Train Loss 130.34241761730985, Validation Loss 14784.171416562811\n",
      "eta 1e-05, lambda 0.01, Train Accuracy 0.9988479262672811, Validation Accuracy 0.5084485407066052, Train Loss 103.9424162041716, Validation Loss 15620.053452084703\n",
      "eta 1e-05, lambda 0.1, Train Accuracy 0.9986559139784946, Validation Accuracy 0.5249615975422427, Train Loss 125.56923422919087, Validation Loss 15061.99201473766\n",
      "eta 1e-05, lambda 1, Train Accuracy 0.9986559139784946, Validation Accuracy 0.5264976958525346, Train Loss 119.39444092120259, Validation Loss 15156.67636380321\n",
      "eta 0.0001, lambda 1e-05, Train Accuracy 0.9993279569892473, Validation Accuracy 0.5506912442396313, Train Loss 48.16129176246225, Validation Loss 14427.702401972172\n",
      "eta 0.0001, lambda 0.0001, Train Accuracy 0.9995199692780338, Validation Accuracy 0.5230414746543779, Train Loss 48.242783083057255, Validation Loss 15290.900803712786\n",
      "eta 0.0001, lambda 0.001, Train Accuracy 0.9995199692780338, Validation Accuracy 0.554531490015361, Train Loss 49.294413802220454, Validation Loss 14355.894577905021\n",
      "eta 0.0001, lambda 0.01, Train Accuracy 0.999231950844854, Validation Accuracy 0.544162826420891, Train Loss 55.56012163200338, Validation Loss 14666.716820248079\n",
      "eta 0.0001, lambda 0.1, Train Accuracy 0.9995199692780338, Validation Accuracy 0.5476190476190477, Train Loss 43.76211418131644, Validation Loss 14429.260881481929\n",
      "eta 0.0001, lambda 1, Train Accuracy 0.9993279569892473, Validation Accuracy 0.5407066052227343, Train Loss 42.83380002981532, Validation Loss 14424.358368418412\n",
      "eta 0.001, lambda 1e-05, Train Accuracy 0.9995199692780338, Validation Accuracy 0.8172043010752689, Train Loss 81.88739369844734, Validation Loss 6051.156654069921\n",
      "eta 0.001, lambda 0.0001, Train Accuracy 0.9993279569892473, Validation Accuracy 0.8248847926267281, Train Loss 97.31372544417705, Validation Loss 5953.973567750279\n",
      "eta 0.001, lambda 0.001, Train Accuracy 0.9996159754224271, Validation Accuracy 0.8187403993855606, Train Loss 63.49356998284631, Validation Loss 6065.8800389554735\n",
      "eta 0.001, lambda 0.01, Train Accuracy 0.9996159754224271, Validation Accuracy 0.8210445468509985, Train Loss 63.49373590484861, Validation Loss 5769.327624264672\n",
      "eta 0.001, lambda 0.1, Train Accuracy 0.9995199692780338, Validation Accuracy 0.8283410138248848, Train Loss 65.21132217997445, Validation Loss 5782.885361869927\n",
      "eta 0.001, lambda 1, Train Accuracy 0.9995199692780338, Validation Accuracy 0.8283410138248848, Train Loss 58.668184446426075, Validation Loss 3140.250257662567\n",
      "eta 0.01, lambda 1e-05, Train Accuracy 0.9994239631336406, Validation Accuracy 0.912826420890937, Train Loss 110.52737936818843, Validation Loss 3879.690515332786\n",
      "eta 0.01, lambda 0.0001, Train Accuracy 0.9994239631336406, Validation Accuracy 0.9047619047619048, Train Loss 101.87544735928542, Validation Loss 4228.995853264426\n",
      "eta 0.01, lambda 0.001, Train Accuracy 0.9995199692780338, Validation Accuracy 0.902073732718894, Train Loss 84.66952734534419, Validation Loss 4435.539929600615\n",
      "eta 0.01, lambda 0.01, Train Accuracy 0.9995199692780338, Validation Accuracy 0.9047619047619048, Train Loss 88.7817314216084, Validation Loss 4317.7159597394075\n",
      "eta 0.01, lambda 0.1, Train Accuracy 0.9995199692780338, Validation Accuracy 0.9039938556067588, Train Loss 92.10480704672787, Validation Loss 4231.29610867807\n",
      "eta 0.01, lambda 1, Train Accuracy 0.9994239631336406, Validation Accuracy 0.9009216589861752, Train Loss 110.67882039424813, Validation Loss 3053.637205502555\n",
      "eta 0.1, lambda 1e-05, Train Accuracy 0.9994239631336406, Validation Accuracy 0.9074500768049155, Train Loss 110.5239803643942, Validation Loss 4435.038114724296\n",
      "eta 0.1, lambda 0.0001, Train Accuracy 0.9990399385560675, Validation Accuracy 0.9009216589861752, Train Loss 184.2067033795248, Validation Loss 4750.10181161341\n",
      "eta 0.1, lambda 0.001, Train Accuracy 0.9996159754224271, Validation Accuracy 0.9059139784946236, Train Loss 73.68261891219248, Validation Loss 4486.576378777186\n",
      "eta 0.1, lambda 0.01, Train Accuracy 0.9995199692780338, Validation Accuracy 0.9051459293394777, Train Loss 92.1036542723404, Validation Loss 4476.54882406229\n",
      "eta 0.1, lambda 0.1, Train Accuracy 0.9994239631336406, Validation Accuracy 0.8951612903225806, Train Loss 110.52733173775087, Validation Loss 4921.521974684071\n",
      "eta 0.1, lambda 1, Train Accuracy 0.9907834101382489, Validation Accuracy 0.8870967741935484, Train Loss 1743.6675778591566, Validation Loss 5387.163910771642\n",
      "eta 1, lambda 1e-05, Train Accuracy 0.9994239631336406, Validation Accuracy 0.9055299539170507, Train Loss 110.52398036371535, Validation Loss 4531.487439432282\n",
      "eta 1, lambda 0.0001, Train Accuracy 0.9996159754224271, Validation Accuracy 0.9086021505376344, Train Loss 73.68261885581062, Validation Loss 4384.121993400663\n",
      "eta 1, lambda 0.001, Train Accuracy 0.9996159754224271, Validation Accuracy 0.8993855606758833, Train Loss 73.68261885581062, Validation Loss 4806.92750949348\n",
      "eta 1, lambda 0.01, Train Accuracy 0.9993279569892473, Validation Accuracy 0.9062980030721967, Train Loss 125.86425460750922, Validation Loss 4451.505188513637\n",
      "eta 1, lambda 0.1, Train Accuracy 0.9986559139784946, Validation Accuracy 0.90284178187404, Train Loss 240.7946540318535, Validation Loss 4658.912432826401\n",
      "eta 1, lambda 1, Train Accuracy 0.10320660522273425, Validation Accuracy 0.09869431643625191, Train Loss 172067.57881850906, Validation Loss 43233.337703486206\n",
      "Best eta: 0.01\n",
      "Best lambda: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# Define the range of values for eta and lambda\n",
    "eta_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "lambda_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "# Initialize lists to store the results\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Loop through all combinations of eta and lambda and perform logistic regression for each combination\n",
    "for eta in eta_values:\n",
    "    for lambda_ in lambda_values:\n",
    "        # Train the logistic regression model\n",
    "        WvChi, lossesvChi, lossesVvChi, accuraciesChi, accuraciesVChi = logistic_regression(X_train_chi, y_train_chi, X_val_chi, y_val_chi, epochs=1000, minEpoch=5, eta=eta, lambda_=lambda_, threshold=1)\n",
    "        \n",
    "        X_train_chi_p = sparse.hstack((np.ones((X_train_chi.shape[0], 1)), X_train_chi))\n",
    "        s = predict(X = X_train_chi_p, W = WvChi)\n",
    "        # Calculate the accuracy and loss on the training set\n",
    "        train_accuracy = accuracy(y_true=np.argmax(y_train_chi, axis=1), y_pred=np.argmax(s, axis=0))\n",
    "        train_loss = calculate_loss(y=y_train_chi, s=s.T)\n",
    "        \n",
    "        X_val_chi_p = sparse.hstack((np.ones((X_val_chi.shape[0], 1)), X_val_chi))\n",
    "        st = predict(X = X_val_chi_p, W = WvChi)\n",
    "        # Calculate the accuracy and loss on the validation set\n",
    "        val_accuracy = accuracy(y_true=np.argmax(y_val_chi, axis=1), y_pred=np.argmax(st, axis=0))\n",
    "        val_loss = calculate_loss(y=y_val_chi, s=st.T)\n",
    "\n",
    "        # Append the results to the lists\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"eta {eta}, lambda {lambda_}, Train Accuracy {train_accuracy}, Validation Accuracy {val_accuracy}, Train Loss {train_loss}, Validation Loss {val_loss}\")\n",
    "\n",
    "# Reshape the lists into matrices with one dimension for each hyperparameter\n",
    "train_accuracies = np.array(train_accuracies).reshape(len(eta_values), len(lambda_values))\n",
    "val_accuracies = np.array(val_accuracies).reshape(len(eta_values), len(lambda_values))\n",
    "train_losses = np.array(train_losses).reshape(len(eta_values), len(lambda_values))\n",
    "val_losses = np.array(val_losses).reshape(len(eta_values), len(lambda_values))\n",
    "\n",
    "# Find the optimal values of eta and lambda that give the best validation accuracy\n",
    "best_eta_index, best_lambda_index = np.unravel_index(np.argmax(val_accuracies), val_accuracies.shape)\n",
    "best_eta = eta_values[best_eta_index]\n",
    "best_lambda = lambda_values[best_lambda_index]\n",
    "\n",
    "print(f\"Best eta: {best_eta}\")\n",
    "print(f\"Best lambda: {best_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd8d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
