{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Categorization\n",
    "\n",
    "## Objectives\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 2, 'class': 16, 'words': array([0, 0, 0, ..., 0, 0, 0])}, {'id': 3, 'class': 8, 'words': array([0, 0, 0, ..., 0, 0, 0])}, {'id': 4, 'class': 11, 'words': array([0, 0, 0, ..., 0, 0, 0])}, {'id': 5, 'class': 12, 'words': array([0, 0, 0, ..., 0, 0, 0])}]\n"
     ]
    }
   ],
   "source": [
    "# Upload data frame\n",
    "\n",
    "data = np.loadtxt(\"training.csv\", dtype='int', delimiter=\",\")\n",
    "df = []\n",
    "\n",
    "for line in data:\n",
    "    df_line = {\"id\": line[0], \"class\": line[61189], \"words\": line[1:61188]}\n",
    "    df.append(df_line)\n",
    "\n",
    "print(df[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '1', 'name': 'alt.atheism'}, {'id': '2', 'name': 'comp.graphics'}, {'id': '3', 'name': 'comp.os.ms-windows.misc'}, {'id': '4', 'name': 'comp.sys.ibm.pc.hardware'}, {'id': '5', 'name': 'comp.sys.mac.hardware'}, {'id': '6', 'name': 'comp.windows.x'}, {'id': '7', 'name': 'misc.forsale'}, {'id': '8', 'name': 'rec.autos'}, {'id': '9', 'name': 'rec.motorcycles'}, {'id': '10', 'name': 'rec.sport.baseball'}, {'id': '11', 'name': 'rec.sport.hockey'}, {'id': '12', 'name': 'sci.crypt'}, {'id': '13', 'name': 'sci.electronics'}, {'id': '14', 'name': 'sci.med'}, {'id': '15', 'name': 'sci.space'}, {'id': '16', 'name': 'soc.religion.christian'}, {'id': '17', 'name': 'talk.politics.guns'}, {'id': '18', 'name': 'talk.politics.mideast'}, {'id': '19', 'name': 'talk.politics.misc'}, {'id': '20', 'name': 'talk.religion.misc'}]\n"
     ]
    }
   ],
   "source": [
    "# Upload classes\n",
    "\n",
    "classes_file = open('newsgrouplabels.txt', 'r')\n",
    "classes = []\n",
    "for line in classes_file:\n",
    "    to_add = line.split()\n",
    "    classes.append({'id': to_add[0], 'name': to_add[1]})\n",
    "    \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload vocabulary\n",
    "\n",
    "vocabulary_file = open('vocabulary.txt', 'r')\n",
    "vocabulary_data = vocabulary_file.read()\n",
    "vocabulary = vocabulary_data.split()\n",
    "\n",
    "# print(len(vocabulary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04025, 0.052, 0.051833333333333335, 0.05358333333333333, 0.050166666666666665, 0.0525, 0.0515, 0.051166666666666666, 0.05408333333333333, 0.052333333333333336, 0.05383333333333333, 0.05325, 0.05216666666666667, 0.05175, 0.05308333333333334, 0.05425, 0.04833333333333333, 0.049416666666666664, 0.03891666666666667, 0.035583333333333335]\n"
     ]
    }
   ],
   "source": [
    "# Calculating MLE\n",
    "\n",
    "mles = [0 for quantity in classes]\n",
    "\n",
    "for doc in df:\n",
    "    mles[doc[\"class\"]-1] += 1\n",
    "\n",
    "for i in range(0, len(mles)):\n",
    "    mles[i] /= len(df)\n",
    "\n",
    "print(mles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     prob_doc \u001b[39m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m doc[\u001b[39m\"\u001b[39m\u001b[39mwords\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m----> 8\u001b[0m         prob_doc_word \u001b[39m=\u001b[39m (word \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39;49munique(doc[\u001b[39m\"\u001b[39;49m\u001b[39mwords\u001b[39;49m\u001b[39m\"\u001b[39;49m] )\u001b[39m+\u001b[39m (beta\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m(\u001b[39mlen\u001b[39m(vocabulary)))\n\u001b[1;32m      9\u001b[0m         prob_doc\u001b[39m.\u001b[39mappend(prob_doc_word)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(maps[\u001b[39m1\u001b[39m:\u001b[39m5\u001b[39m])\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/topic-categorization/lib/python3.11/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/topic-categorization/lib/python3.11/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[39m.\u001b[39msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[39m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculating MAP\n",
    "# P(Xi | Yk) = ((count of Xi in Yk) + (1 - alpha))/ ((total words in Yk)+ ((1/alpha)*(length of vocab list)))\n",
    "maps = []\n",
    "beta = 1 / len(vocabulary)\n",
    "alpha = 1 + beta\n",
    "for doc in df:\n",
    "    prob_doc = []\n",
    "    for word in doc[\"words\"]:\n",
    "        prob_doc_word = (word + alpha - 1) / (np.unique(doc[\"words\"]) + (alpha-1)*(len(vocabulary)))\n",
    "        prob_doc.append(prob_doc_word)\n",
    "\n",
    "print(maps[1:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the probabilities we can now make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert prediction things, adding input\n",
    "sentence = \"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic-categorization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
